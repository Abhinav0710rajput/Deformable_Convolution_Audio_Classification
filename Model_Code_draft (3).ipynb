{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "74ccf733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from timm.models.layers import trunc_normal_, DropPath\n",
    "from ops_dcnv3 import modules as opsm\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2e83eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class to_channels_first(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "class to_channels_last(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.permute(0, 2, 3, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76703f86",
   "metadata": {},
   "source": [
    "# Normalisation layer, activation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6572bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_norm_layer(dim,\n",
    "                     norm_layer,\n",
    "                     in_format='channels_last',\n",
    "                     out_format='channels_last',\n",
    "                     eps=1e-6):\n",
    "    layers = []\n",
    "    if norm_layer == 'BN':\n",
    "        if in_format == 'channels_last':\n",
    "            layers.append(to_channels_first())\n",
    "        layers.append(nn.BatchNorm2d(dim))\n",
    "        if out_format == 'channels_last':\n",
    "            layers.append(to_channels_last())\n",
    "    elif norm_layer == 'LN':\n",
    "        if in_format == 'channels_first':\n",
    "            layers.append(to_channels_last())\n",
    "        layers.append(nn.LayerNorm(dim, eps=eps))\n",
    "        if out_format == 'channels_first':\n",
    "            layers.append(to_channels_first())\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f'build_norm_layer does not support {norm_layer}')\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def build_act_layer(act_layer):\n",
    "    if act_layer == 'ReLU':\n",
    "        return nn.ReLU(inplace=True)\n",
    "    elif act_layer == 'SiLU':\n",
    "        return nn.SiLU(inplace=True)\n",
    "    elif act_layer == 'GELU':\n",
    "        return nn.GELU()\n",
    "\n",
    "    raise NotImplementedError(f'build_act_layer does not support {act_layer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c292a8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    r\"\"\" Cross Attention Module\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        num_heads (int): Number of attention heads. Default: 8\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to q, k, v.\n",
    "            Default: False.\n",
    "        qk_scale (float | None, optional): Override default qk scale of\n",
    "            head_dim ** -0.5 if set. Default: None.\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight.\n",
    "            Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "        attn_head_dim (int, optional): Dimension of attention head.\n",
    "        out_dim (int, optional): Dimension of output.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads=8,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 attn_drop=0.,\n",
    "                 proj_drop=0.,\n",
    "                 attn_head_dim=None,\n",
    "                 out_dim=None):\n",
    "        super().__init__()\n",
    "        if out_dim is None:\n",
    "            out_dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        if attn_head_dim is not None:\n",
    "            head_dim = attn_head_dim\n",
    "        all_head_dim = head_dim * self.num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        assert all_head_dim == dim\n",
    "\n",
    "        self.q = nn.Linear(dim, all_head_dim, bias=False)\n",
    "        self.k = nn.Linear(dim, all_head_dim, bias=False)\n",
    "        self.v = nn.Linear(dim, all_head_dim, bias=False)\n",
    "\n",
    "        if qkv_bias:\n",
    "            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "            self.k_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "        else:\n",
    "            self.q_bias = None\n",
    "            self.k_bias = None\n",
    "            self.v_bias = None\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(all_head_dim, out_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x, k=None, v=None):\n",
    "        B, N, C = x.shape\n",
    "        N_k = k.shape[1]\n",
    "        N_v = v.shape[1]\n",
    "\n",
    "        q_bias, k_bias, v_bias = None, None, None\n",
    "        if self.q_bias is not None:\n",
    "            q_bias = self.q_bias\n",
    "            k_bias = self.k_bias\n",
    "            v_bias = self.v_bias\n",
    "\n",
    "        q = F.linear(input=x, weight=self.q.weight, bias=q_bias)\n",
    "        q = q.reshape(B, N, 1, self.num_heads,\n",
    "                      -1).permute(2, 0, 3, 1,\n",
    "                                  4).squeeze(0)  # (B, N_head, N_q, dim)\n",
    "\n",
    "        k = F.linear(input=k, weight=self.k.weight, bias=k_bias)\n",
    "        k = k.reshape(B, N_k, 1, self.num_heads, -1).permute(2, 0, 3, 1,\n",
    "                                                             4).squeeze(0)\n",
    "\n",
    "        v = F.linear(input=v, weight=self.v.weight, bias=v_bias)\n",
    "        v = v.reshape(B, N_v, 1, self.num_heads, -1).permute(2, 0, 3, 1,\n",
    "                                                             4).squeeze(0)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))  # (B, N_head, N_q, N_k)\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentiveBlock(nn.Module):\n",
    "    r\"\"\"Attentive Block\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        num_heads (int): Number of attention heads. Default: 8\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to q, k, v.\n",
    "            Default: False.\n",
    "        qk_scale (float | None, optional): Override default qk scale of\n",
    "            head_dim ** -0.5 if set. Default: None.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0.\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0.\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate.\n",
    "            Default: 0.0.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm.\n",
    "        attn_head_dim (int, optional): Dimension of attention head. Default: None.\n",
    "        out_dim (int, optional): Dimension of output. Default: None.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 norm_layer=\"LN\",\n",
    "                 attn_head_dim=None,\n",
    "                 out_dim=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1_q = build_norm_layer(dim, norm_layer, eps=1e-6)\n",
    "        self.norm1_k = build_norm_layer(dim, norm_layer, eps=1e-6)\n",
    "        self.norm1_v = build_norm_layer(dim, norm_layer, eps=1e-6)\n",
    "        self.cross_dcn = CrossAttention(dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        qkv_bias=qkv_bias,\n",
    "                                        qk_scale=qk_scale,\n",
    "                                        attn_drop=attn_drop,\n",
    "                                        proj_drop=drop,\n",
    "                                        attn_head_dim=attn_head_dim,\n",
    "                                        out_dim=out_dim)\n",
    "\n",
    "        self.drop_path = DropPath(\n",
    "            drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self,\n",
    "                x_q,\n",
    "                x_kv,\n",
    "                pos_q,\n",
    "                pos_k,\n",
    "                bool_masked_pos,\n",
    "                rel_pos_bias=None):\n",
    "        x_q = self.norm1_q(x_q + pos_q)\n",
    "        x_k = self.norm1_k(x_kv + pos_k)\n",
    "        x_v = self.norm1_v(x_kv)\n",
    "\n",
    "        x = self.cross_dcn(x_q, k=x_k, v=x_v)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionPoolingBlock(AttentiveBlock):\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_q = x.mean(1, keepdim=True)\n",
    "        x_kv = x\n",
    "        pos_q, pos_k = 0, 0\n",
    "        x = super().forward(x_q, x_kv, pos_q, pos_k,\n",
    "                            bool_masked_pos=None,\n",
    "                            rel_pos_bias=None)\n",
    "        x = x.squeeze(1)\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49fe03e4",
   "metadata": {},
   "source": [
    "# Stemming layer, used once intially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d0909917",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemLayer(nn.Module):\n",
    "    r\"\"\" Stem layer of InternImage\n",
    "    Args:\n",
    "        in_chans (int): number of input channels\n",
    "        out_chans (int): number of output channels\n",
    "        act_layer (str): activation layer\n",
    "        norm_layer (str): normalization layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_chans=3,\n",
    "                 out_chans=96,\n",
    "                 act_layer='GELU',\n",
    "                 norm_layer='BN'):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_chans,\n",
    "                               out_chans // 2,\n",
    "                               kernel_size=3,\n",
    "                               stride=2,\n",
    "                               padding=1)\n",
    "        self.norm1 = build_norm_layer(out_chans // 2, norm_layer,\n",
    "                                      'channels_first', 'channels_first')\n",
    "        self.act = build_act_layer(act_layer)\n",
    "        self.conv2 = nn.Conv2d(out_chans // 2,\n",
    "                               out_chans,\n",
    "                               kernel_size=3,\n",
    "                               stride=2,\n",
    "                               padding=1)\n",
    "        self.norm2 = build_norm_layer(out_chans, norm_layer, 'channels_first',\n",
    "                                      'channels_last')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64d24b87",
   "metadata": {},
   "source": [
    "# Down sampling layer, mlp layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a7505d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownsampleLayer(nn.Module):\n",
    "    r\"\"\" Downsample layer of InternImage\n",
    "    Args:\n",
    "        channels (int): number of input channels\n",
    "        norm_layer (str): normalization layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, norm_layer='LN'):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channels,\n",
    "                              2 * channels,\n",
    "                              kernel_size=3,\n",
    "                              stride=2,\n",
    "                              padding=1,\n",
    "                              bias=False)\n",
    "        self.norm = build_norm_layer(2 * channels, norm_layer,\n",
    "                                     'channels_first', 'channels_last')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x.permute(0, 3, 1, 2))\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLPLayer(nn.Module):\n",
    "    r\"\"\" MLP layer of InternImage\n",
    "    Args:\n",
    "        in_features (int): number of input features\n",
    "        hidden_features (int): number of hidden features\n",
    "        out_features (int): number of output features\n",
    "        act_layer (str): activation layer\n",
    "        drop (float): dropout rate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 hidden_features=None,\n",
    "                 out_features=None,\n",
    "                 act_layer='GELU',\n",
    "                 drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = build_act_layer(act_layer)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47b5b689",
   "metadata": {},
   "source": [
    "# InternImage Layer (BASIC BLOCK) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "99adc226",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InternImageLayer(nn.Module):\n",
    "    r\"\"\" Basic layer of InternImage\n",
    "    Args:\n",
    "        core_op (nn.Module): core operation of InternImage\n",
    "        channels (int): number of input channels\n",
    "        groups (list): Groups of each block.\n",
    "        mlp_ratio (float): ratio of mlp hidden features to input channels\n",
    "        drop (float): dropout rate\n",
    "        drop_path (float): drop path rate\n",
    "        act_layer (str): activation layer\n",
    "        norm_layer (str): normalization layer\n",
    "        post_norm (bool): whether to use post normalization\n",
    "        layer_scale (float): layer scale\n",
    "        offset_scale (float): offset scale\n",
    "        with_cp (bool): whether to use checkpoint\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 core_op,\n",
    "                 channels,           #SAME THROUGHOUT\n",
    "                 groups,\n",
    "                 mlp_ratio=4.,\n",
    "                 drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 act_layer='GELU',\n",
    "                 norm_layer='LN',\n",
    "                 post_norm=False,\n",
    "                 layer_scale=None,\n",
    "                 offset_scale=1.0,\n",
    "                 with_cp=False,\n",
    "                 dw_kernel_size=None, \n",
    "                 res_post_norm=False,\n",
    "                 center_feature_scale=False, \n",
    "                 remove_center=False, \n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.groups = groups\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.with_cp = with_cp\n",
    "\n",
    "        self.norm1 = build_norm_layer(channels, 'LN')\n",
    "        self.post_norm = post_norm\n",
    "        self.dcn = core_op(\n",
    "            channels=channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            pad=1,\n",
    "            dilation=1,\n",
    "            group=groups,\n",
    "            offset_scale=offset_scale,\n",
    "            act_layer=act_layer,\n",
    "            norm_layer=norm_layer,\n",
    "            dw_kernel_size=dw_kernel_size,\n",
    "            center_feature_scale=center_feature_scale,\n",
    "            remove_center=remove_center,  \n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. \\\n",
    "            else nn.Identity()\n",
    "        self.norm2 = build_norm_layer(channels, 'LN')\n",
    "        self.mlp = MLPLayer(in_features=channels,\n",
    "                            hidden_features=int(channels * mlp_ratio),\n",
    "                            act_layer=act_layer,\n",
    "                            drop=drop)\n",
    "        self.layer_scale = layer_scale is not None\n",
    "        if self.layer_scale:\n",
    "            self.gamma1 = nn.Parameter(layer_scale * torch.ones(channels),\n",
    "                                       requires_grad=True)\n",
    "            self.gamma2 = nn.Parameter(layer_scale * torch.ones(channels),\n",
    "                                       requires_grad=True)\n",
    "        self.res_post_norm = res_post_norm\n",
    "        if res_post_norm:\n",
    "            self.res_post_norm1 = build_norm_layer(channels, 'LN')\n",
    "            self.res_post_norm2 = build_norm_layer(channels, 'LN')\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        def _inner_forward(x):\n",
    "            if not self.layer_scale:\n",
    "                if self.post_norm:\n",
    "                    x = x + self.drop_path(self.norm1(self.dcn(x)))\n",
    "                    x = x + self.drop_path(self.norm2(self.mlp(x)))\n",
    "                elif self.res_post_norm: \n",
    "                    x = x + self.drop_path(self.res_post_norm1(self.dcn(self.norm1(x))))\n",
    "                    x = x + self.drop_path(self.res_post_norm2(self.mlp(self.norm2(x))))\n",
    "                else:\n",
    "                    x = x + self.drop_path(self.dcn(self.norm1(x)))\n",
    "                    x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "                return x\n",
    "            if self.post_norm:\n",
    "                x = x + self.drop_path(self.gamma1 * self.norm1(self.dcn(x)))\n",
    "                x = x + self.drop_path(self.gamma2 * self.norm2(self.mlp(x)))\n",
    "            else:\n",
    "                x = x + self.drop_path(self.gamma1 * self.dcn(self.norm1(x)))\n",
    "                x = x + self.drop_path(self.gamma2 * self.mlp(self.norm2(x)))\n",
    "            return x\n",
    "\n",
    "        if self.with_cp and x.requires_grad:\n",
    "            x = checkpoint.checkpoint(_inner_forward, x)\n",
    "        else:\n",
    "            x = _inner_forward(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6dab6d54",
   "metadata": {},
   "source": [
    "# Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4467c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InternImageBlock(nn.Module):\n",
    "    r\"\"\" Block of InternImage\n",
    "    Args:\n",
    "        core_op (nn.Module): core operation of InternImage\n",
    "        channels (int): number of input channels\n",
    "        depths (list): Depth of each block.\n",
    "        groups (list): Groups of each block.\n",
    "        mlp_ratio (float): ratio of mlp hidden features to input channels\n",
    "        drop (float): dropout rate\n",
    "        drop_path (float): drop path rate\n",
    "        act_layer (str): activation layer\n",
    "        norm_layer (str): normalization layer\n",
    "        post_norm (bool): whether to use post normalization\n",
    "        layer_scale (float): layer scale\n",
    "        offset_scale (float): offset scale\n",
    "        with_cp (bool): whether to use checkpoint\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 core_op,\n",
    "                 channels,\n",
    "                 depth,\n",
    "                 groups,                                       # GROUP NUMBER  OF DCNV3 FOR A SEGMENT i\n",
    "                 downsample=True,\n",
    "                 mlp_ratio=4.,\n",
    "                 drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 act_layer='GELU',\n",
    "                 norm_layer='LN',\n",
    "                 post_norm=False,\n",
    "                 offset_scale=1.0,\n",
    "                 layer_scale=None,\n",
    "                 with_cp=False,\n",
    "                 dw_kernel_size=None, \n",
    "                 post_norm_block_ids=None, \n",
    "                 res_post_norm=False, \n",
    "                 center_feature_scale=False, \n",
    "                 remove_center=False, \n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.depth = depth\n",
    "        self.post_norm = post_norm\n",
    "        self.center_feature_scale = center_feature_scale\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            InternImageLayer(\n",
    "                core_op=core_op,\n",
    "                channels=channels,\n",
    "                groups=groups,      \n",
    "                mlp_ratio=mlp_ratio,\n",
    "                drop=drop,\n",
    "                drop_path=drop_path[i] if isinstance(\n",
    "                    drop_path, list) else drop_path,\n",
    "                act_layer=act_layer,\n",
    "                norm_layer=norm_layer,\n",
    "                post_norm=post_norm,\n",
    "                layer_scale=layer_scale,\n",
    "                offset_scale=offset_scale,\n",
    "                with_cp=with_cp,\n",
    "                dw_kernel_size=dw_kernel_size, \n",
    "                res_post_norm=res_post_norm, \n",
    "                center_feature_scale=center_feature_scale,\n",
    "                remove_center = remove_center,  \n",
    "        ) for i in range(depth)                                 #NUMBER OF BASIC BLOCKS IN A SEGMENT\n",
    "        ])\n",
    "        if not self.post_norm or center_feature_scale:\n",
    "            self.norm = build_norm_layer(channels, 'LN')\n",
    "        self.post_norm_block_ids = post_norm_block_ids\n",
    "        if post_norm_block_ids is not None:\n",
    "            self.post_norms = nn.ModuleList(\n",
    "                [build_norm_layer(channels, 'LN', eps=1e-6) for _ in post_norm_block_ids]\n",
    "            )\n",
    "        self.downsample = DownsampleLayer(\n",
    "            channels=channels, norm_layer=norm_layer) if downsample else None\n",
    "\n",
    "    def forward(self, x, return_wo_downsample=False):\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "            if (self.post_norm_block_ids is not None) and (i in self.post_norm_block_ids):\n",
    "                index = self.post_norm_block_ids.index(i)\n",
    "                x = self.post_norms[index](x) \n",
    "        if not self.post_norm or self.center_feature_scale:\n",
    "            x = self.norm(x)\n",
    "        if return_wo_downsample:\n",
    "            x_ = x\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "\n",
    "        if return_wo_downsample:\n",
    "            return x, x_\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12d56fe0",
   "metadata": {},
   "source": [
    "# Complete Arch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cece4f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InternImage(nn.Module):\n",
    "    r\"\"\" InternImage\n",
    "        A PyTorch impl of : `InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions`  -\n",
    "          https://arxiv.org/pdf/2103.14030\n",
    "    Args:\n",
    "        core_op (str): Core operator. Default: 'DCNv3'\n",
    "        channels (int): Number of the first stage. Default: 64\n",
    "        depths (list): Depth of each block. Default: [3, 4, 18, 5]\n",
    "        groups (list): Groups of each block. Default: [3, 6, 12, 24]\n",
    "        num_classes (int): Number of classes. Default: 1000\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n",
    "        drop_rate (float): Probability of an element to be zeroed. Default: 0.\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.\n",
    "        act_layer (str): Activation layer. Default: 'GELU'\n",
    "        norm_layer (str): Normalization layer. Default: 'LN'\n",
    "        layer_scale (bool): Whether to use layer scale. Default: False\n",
    "        cls_scale (bool): Whether to use class scale. Default: False\n",
    "        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n",
    "        dw_kernel_size (int): Size of the dwconv. Default: None\n",
    "        use_clip_projector (bool): Whether to use clip projector. Default: False\n",
    "        level2_post_norm (bool): Whether to use level2 post norm. Default: False\n",
    "        level2_post_norm_block_ids (list): Indexes of post norm blocks. Default: None\n",
    "        res_post_norm (bool): Whether to use res post norm. Default: False\n",
    "        center_feature_scale (bool): Whether to use center feature scale. Default: False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 core_op='DCNv3',\n",
    "                 channels=64,\n",
    "                 depths=[3, 4, 18, 5],               # Li\n",
    "                 groups=[4, 8, 16, 32],              # Gi = Ci/(C'= constant)\n",
    "                 num_classes=1000,\n",
    "                 mlp_ratio=4.,\n",
    "                 drop_rate=0.,\n",
    "                 drop_path_rate=0.2,\n",
    "                 drop_path_type='linear',\n",
    "                 act_layer='GELU',\n",
    "                 norm_layer='LN',\n",
    "                 layer_scale=None,\n",
    "                 offset_scale=1.0,\n",
    "                 post_norm=False,\n",
    "                 cls_scale=1.5,\n",
    "                 with_cp=False,\n",
    "                 dw_kernel_size=None, \n",
    "                 use_clip_projector=False, \n",
    "                 level2_post_norm=False, \n",
    "                 level2_post_norm_block_ids=None,\n",
    "                 res_post_norm=False, \n",
    "                 center_feature_scale=False,\n",
    "                 remove_center=False, \n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.core_op = core_op\n",
    "        self.num_classes = num_classes\n",
    "        self.num_levels = len(depths)        #NUMBER OF STAGES , HERE = 4\n",
    "        self.depths = depths\n",
    "        self.channels = channels\n",
    "        self.num_features = int(channels * 2**(self.num_levels - 1))\n",
    "        self.post_norm = post_norm\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.use_clip_projector = use_clip_projector\n",
    "        self.level2_post_norm_block_ids = level2_post_norm_block_ids\n",
    "        self.remove_center = remove_center\n",
    "\n",
    "        print(f'using core type: {core_op}')\n",
    "        print(f'using activation layer: {act_layer}')\n",
    "        print(f'using main norm layer: {norm_layer}')\n",
    "        print(f'using dpr: {drop_path_type}, {drop_path_rate}')\n",
    "        print(f\"level2_post_norm: {level2_post_norm}\")\n",
    "        print(f\"level2_post_norm_block_ids: {level2_post_norm_block_ids}\")\n",
    "        print(f\"res_post_norm: {res_post_norm}\")\n",
    "        print(f\"remove_center: {remove_center}\")\n",
    "\n",
    "        in_chans = 1                                         #INPUT CHANNELS\n",
    "        self.patch_embed = StemLayer(in_chans=in_chans,\n",
    "                                     out_chans=channels,\n",
    "                                     act_layer=act_layer,\n",
    "                                     norm_layer=norm_layer)\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [\n",
    "            x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))\n",
    "        ]\n",
    "        if drop_path_type == 'uniform':\n",
    "            for i in range(len(dpr)):\n",
    "                dpr[i] = drop_path_rate\n",
    "\n",
    "        self.levels = nn.ModuleList()\n",
    "        for i in range(self.num_levels):                                 # SEGMENT CREATION!\n",
    "            post_norm_block_ids = level2_post_norm_block_ids if level2_post_norm and (\n",
    "                i == 2) else None \n",
    "            level = InternImageBlock(\n",
    "                core_op=getattr(opsm, core_op),\n",
    "                channels=int(channels * 2**i),                          # STACKING RULE\n",
    "                depth=depths[i],\n",
    "                groups=groups[i],\n",
    "                mlp_ratio=self.mlp_ratio,\n",
    "                drop=drop_rate,\n",
    "                drop_path=dpr[sum(depths[:i]):sum(depths[:i + 1])],\n",
    "                act_layer=act_layer,\n",
    "                norm_layer=norm_layer,\n",
    "                post_norm=post_norm,\n",
    "                downsample=(i < self.num_levels - 1),                   #NO DOWNSAMPLING FOR LAST LAYER                  \n",
    "                layer_scale=layer_scale,\n",
    "                offset_scale=offset_scale,\n",
    "                with_cp=with_cp,\n",
    "                dw_kernel_size=dw_kernel_size,  \n",
    "                post_norm_block_ids=post_norm_block_ids, \n",
    "                res_post_norm=res_post_norm, \n",
    "                center_feature_scale=center_feature_scale, \n",
    "                remove_center=remove_center,  \n",
    "            )\n",
    "            self.levels.append(level)\n",
    "        \n",
    "        if not use_clip_projector: \n",
    "            self.conv_head = nn.Sequential(\n",
    "                nn.Conv2d(self.num_features,\n",
    "                          int(self.num_features * cls_scale),\n",
    "                          kernel_size=1,\n",
    "                          bias=False),\n",
    "                build_norm_layer(int(self.num_features * cls_scale), 'BN',\n",
    "                                 'channels_first', 'channels_first'),\n",
    "                build_act_layer(act_layer))\n",
    "            self.head = nn.Linear(int(self.num_features * cls_scale), num_classes) \\\n",
    "                if num_classes > 0 else nn.Identity()\n",
    "        else: # for InternImage-H/G\n",
    "            pretrain_embed_dim, _stride, attnpool_num_heads, clip_embed_dim = 1024, 2, 16, 768\n",
    "            self.dcnv3_head_x4 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=self.num_features,\n",
    "                          out_channels=pretrain_embed_dim * (_stride ** 2),\n",
    "                          kernel_size=1), nn.PixelShuffle(_stride))\n",
    "            self.dcnv3_head_x3 = nn.Conv2d(in_channels=self.num_features // 2,\n",
    "                                           out_channels=pretrain_embed_dim,\n",
    "                                           kernel_size=1)\n",
    "            self.clip_projector = AttentionPoolingBlock(\n",
    "                dim=pretrain_embed_dim,\n",
    "                num_heads=attnpool_num_heads,\n",
    "                qkv_bias=True,\n",
    "                qk_scale=None,\n",
    "                drop=0.,\n",
    "                attn_drop=0.,\n",
    "                norm_layer=norm_layer,\n",
    "                out_dim=clip_embed_dim)\n",
    "            self.fc_norm = build_norm_layer(clip_embed_dim, norm_layer, eps=1e-6)\n",
    "            self.head = nn.Linear(\n",
    "                clip_embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "            \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.num_layers = len(depths)\n",
    "        self.apply(self._init_weights)\n",
    "        self.apply(self._init_deform_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def _init_deform_weights(self, m):\n",
    "        if isinstance(m, getattr(opsm, self.core_op)):\n",
    "            m._reset_parameters()\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def lr_decay_keywards(self, decay_ratio=0.87):\n",
    "        lr_ratios = {}\n",
    "\n",
    "        # blocks\n",
    "        idx = 0\n",
    "        for i in range(4):\n",
    "            layer_num = 3 - i  # 3 2 1 0\n",
    "            for j in range(self.depths[layer_num]):\n",
    "                block_num = self.depths[layer_num] - j - 1\n",
    "                tag = 'levels.{}.blocks.{}.'.format(layer_num, block_num)\n",
    "                decay = 1.0 * (decay_ratio**idx)\n",
    "                lr_ratios[tag] = decay\n",
    "                idx += 1\n",
    "        # patch_embed (before stage-1)\n",
    "        lr_ratios[\"patch_embed\"] = lr_ratios['levels.0.blocks.0.']\n",
    "        # levels.0.downsample (between stage-1 and stage-2)\n",
    "        lr_ratios[\"levels.0.downsample\"] = lr_ratios['levels.1.blocks.0.']\n",
    "        lr_ratios[\"levels.0.norm\"] = lr_ratios['levels.1.blocks.0.']\n",
    "        # levels.1.downsample (between stage-2 and stage-3)\n",
    "        lr_ratios[\"levels.1.downsample\"] = lr_ratios['levels.2.blocks.0.']\n",
    "        lr_ratios[\"levels.1.norm\"] = lr_ratios['levels.2.blocks.0.']\n",
    "        # levels.2.downsample (between stage-3 and stage-4)\n",
    "        lr_ratios[\"levels.2.downsample\"] = lr_ratios['levels.3.blocks.0.']\n",
    "        lr_ratios[\"levels.2.norm\"] = lr_ratios['levels.3.blocks.0.']\n",
    "        return lr_ratios\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)                             #STEM LAYER\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for level in self.levels:\n",
    "            x = level(x)\n",
    "\n",
    "        x = self.conv_head(x.permute(0, 3, 1, 2))\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def forward_features_seq_out(self, x):\n",
    "        x = self.patch_embed(x)                          \n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        seq_out = []\n",
    "        for level in self.levels:\n",
    "            x, x_ = level(x, return_wo_downsample=True)\n",
    "            seq_out.append(x_)\n",
    "        return seq_out\n",
    "    \n",
    "    def forward_clip_projector(self, x):\n",
    "        xs = self.forward_features_seq_out(x)\n",
    "        x1, x2, x3, x4 = xs\n",
    "        \n",
    "        x1 = x1.permute(0, 3, 1, 2) # NHWC -> NCHW\n",
    "        x2 = x2.permute(0, 3, 1, 2) # NHWC -> NCHW\n",
    "        x3 = x3.permute(0, 3, 1, 2) # NHWC -> NCHW\n",
    "        x4 = x4.permute(0, 3, 1, 2) # NHWC -> NCHW\n",
    "\n",
    "        x4 = self.dcnv3_head_x4(x4)\n",
    "        x = x4\n",
    "        x3 = self.dcnv3_head_x3(x3)\n",
    "        x = x + x3\n",
    "\n",
    "        x = x.flatten(-2).transpose(1, 2).contiguous()\n",
    "        x = self.clip_projector(x)\n",
    "        x = self.fc_norm(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d9d066b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using core type: DCNv3\n",
      "using activation layer: GELU\n",
      "using main norm layer: LN\n",
      "using dpr: linear, 0.2\n",
      "level2_post_norm: False\n",
      "level2_post_norm_block_ids: None\n",
      "res_post_norm: False\n",
      "remove_center: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "InternImage(\n",
       "  (patch_embed): StemLayer(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (norm1): Sequential(\n",
       "      (0): to_channels_last()\n",
       "      (1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
       "      (2): to_channels_first()\n",
       "    )\n",
       "    (act): GELU(approximate='none')\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (norm2): Sequential(\n",
       "      (0): to_channels_last()\n",
       "      (1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (levels): ModuleList(\n",
       "    (0): InternImageBlock(\n",
       "      (blocks): ModuleList(\n",
       "        (0): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=64, out_features=72, bias=True)\n",
       "            (mask): Linear(in_features=64, out_features=36, bias=True)\n",
       "            (input_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (output_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=64, out_features=72, bias=True)\n",
       "            (mask): Linear(in_features=64, out_features=36, bias=True)\n",
       "            (input_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (output_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.007)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=64, out_features=72, bias=True)\n",
       "            (mask): Linear(in_features=64, out_features=36, bias=True)\n",
       "            (input_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (output_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.014)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): Sequential(\n",
       "        (0): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (downsample): DownsampleLayer(\n",
       "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (norm): Sequential(\n",
       "          (0): to_channels_last()\n",
       "          (1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): InternImageBlock(\n",
       "      (blocks): ModuleList(\n",
       "        (0): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=128, out_features=144, bias=True)\n",
       "            (mask): Linear(in_features=128, out_features=72, bias=True)\n",
       "            (input_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (output_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.021)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=128, out_features=144, bias=True)\n",
       "            (mask): Linear(in_features=128, out_features=72, bias=True)\n",
       "            (input_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (output_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.028)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=128, out_features=144, bias=True)\n",
       "            (mask): Linear(in_features=128, out_features=72, bias=True)\n",
       "            (input_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (output_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.034)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=128, out_features=144, bias=True)\n",
       "            (mask): Linear(in_features=128, out_features=72, bias=True)\n",
       "            (input_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (output_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.041)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): Sequential(\n",
       "        (0): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (downsample): DownsampleLayer(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (norm): Sequential(\n",
       "          (0): to_channels_last()\n",
       "          (1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): InternImageBlock(\n",
       "      (blocks): ModuleList(\n",
       "        (0): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=256, out_features=288, bias=True)\n",
       "            (mask): Linear(in_features=256, out_features=144, bias=True)\n",
       "            (input_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.048)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=256, out_features=288, bias=True)\n",
       "            (mask): Linear(in_features=256, out_features=144, bias=True)\n",
       "            (input_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.055)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=256, out_features=288, bias=True)\n",
       "            (mask): Linear(in_features=256, out_features=144, bias=True)\n",
       "            (input_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.062)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=256, out_features=288, bias=True)\n",
       "            (mask): Linear(in_features=256, out_features=144, bias=True)\n",
       "            (input_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.069)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=256, out_features=288, bias=True)\n",
       "            (mask): Linear(in_features=256, out_features=144, bias=True)\n",
       "            (input_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.076)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=256, out_features=288, bias=True)\n",
       "            (mask): Linear(in_features=256, out_features=144, bias=True)\n",
       "            (input_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.083)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=256, out_features=288, bias=True)\n",
       "            (mask): Linear(in_features=256, out_features=144, bias=True)\n",
       "            (input_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.090)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=256, out_features=288, bias=True)\n",
       "            (mask): Linear(in_features=256, out_features=144, bias=True)\n",
       "            (input_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.097)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=256, out_features=288, bias=True)\n",
       "            (mask): Linear(in_features=256, out_features=144, bias=True)\n",
       "            (input_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.103)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=256, out_features=288, bias=True)\n",
       "            (mask): Linear(in_features=256, out_features=144, bias=True)\n",
       "            (input_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.110)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=256, out_features=288, bias=True)\n",
       "            (mask): Linear(in_features=256, out_features=144, bias=True)\n",
       "            (input_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.117)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=256, out_features=288, bias=True)\n",
       "            (mask): Linear(in_features=256, out_features=144, bias=True)\n",
       "            (input_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.124)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=256, out_features=288, bias=True)\n",
       "            (mask): Linear(in_features=256, out_features=144, bias=True)\n",
       "            (input_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.131)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=256, out_features=288, bias=True)\n",
       "            (mask): Linear(in_features=256, out_features=144, bias=True)\n",
       "            (input_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.138)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=256, out_features=288, bias=True)\n",
       "            (mask): Linear(in_features=256, out_features=144, bias=True)\n",
       "            (input_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.145)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=256, out_features=288, bias=True)\n",
       "            (mask): Linear(in_features=256, out_features=144, bias=True)\n",
       "            (input_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.152)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=256, out_features=288, bias=True)\n",
       "            (mask): Linear(in_features=256, out_features=144, bias=True)\n",
       "            (input_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.159)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=256, out_features=288, bias=True)\n",
       "            (mask): Linear(in_features=256, out_features=144, bias=True)\n",
       "            (input_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.166)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): Sequential(\n",
       "        (0): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (downsample): DownsampleLayer(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (norm): Sequential(\n",
       "          (0): to_channels_last()\n",
       "          (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): InternImageBlock(\n",
       "      (blocks): ModuleList(\n",
       "        (0): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=512, out_features=576, bias=True)\n",
       "            (mask): Linear(in_features=512, out_features=288, bias=True)\n",
       "            (input_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (output_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.172)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=512, out_features=576, bias=True)\n",
       "            (mask): Linear(in_features=512, out_features=288, bias=True)\n",
       "            (input_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (output_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.179)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=512, out_features=576, bias=True)\n",
       "            (mask): Linear(in_features=512, out_features=288, bias=True)\n",
       "            (input_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (output_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.186)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=512, out_features=576, bias=True)\n",
       "            (mask): Linear(in_features=512, out_features=288, bias=True)\n",
       "            (input_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (output_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.193)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): InternImageLayer(\n",
       "          (norm1): Sequential(\n",
       "            (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (dcn): DCNv3(\n",
       "            (dw_conv): Sequential(\n",
       "              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              (1): Sequential(\n",
       "                (0): to_channels_last()\n",
       "                (1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "              (2): GELU(approximate='none')\n",
       "            )\n",
       "            (offset): Linear(in_features=512, out_features=576, bias=True)\n",
       "            (mask): Linear(in_features=512, out_features=288, bias=True)\n",
       "            (input_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (output_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.200)\n",
       "          (norm2): Sequential(\n",
       "            (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (mlp): MLPLayer(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): Sequential(\n",
       "        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_head): Sequential(\n",
       "    (0): Conv2d(512, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): Sequential(\n",
       "      (0): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): GELU(approximate='none')\n",
       "  )\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = InternImage()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9f4358d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.zeros(10, 200, 80)  # AUDIO TENSOR -> (BATCH SIZE, TIME, FREQ)\n",
    "dummy = dummy.unsqueeze(1)\n",
    "#EXPECTED = (BATCH, CHANNELS, HEIGHT, WIDTH)\n",
    "\n",
    "outputs = model(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3eb91819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#____________BEGIN TRAINING________________#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d3a216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba063216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
